{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global and Local Normalization**\n",
    "---\n",
    "\n",
    "言語モデルについてもう少し理論的に深掘りするよという章。\n",
    "\n",
    "**用語についてのメモ**\n",
    "\n",
    "- $\\sum_{\\boldsymbol{v} \\in \\Sigma^*} p_{\\mathrm{LM}}(\\boldsymbol{y})=1$ のような言語モデルならtight.\n",
    "- $\\sum_{\\boldsymbol{v} \\in \\Sigma^*} p_{\\mathrm{LM}}(\\boldsymbol{y})\\leq 1$ならnon-tight\n",
    "\n",
    "この本ではnon-tightなものは言語モデルとは見做さないで、話すらしい、一般的なnon-tightの例はRNNベースの言語モデルをback-propで学習すると、[non-tight](https://aclanthology.org/N18-1205/)になるらしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "これまでの章で、言語モデルには大きく分けて、2種類があることを確認できた。\n",
    "\n",
    "- 文章を受け取り確率を出すもの (Global Model)\n",
    "- 単語ごとに確率を出すもの (Local Model)\n",
    "\n",
    "　これらはtightになることを保証することは正規化 (sumして割る)をしてもできるよということを以下で説明する。\n",
    "\n",
    "\n",
    "\n",
    "**Globally Normalized Language Models**\n",
    "\n",
    "Global Modelで正規化したものを globally normalized models または energy-based language modelという。\n",
    "energy functionとglobally normalized modelsの定義は以下のようになる。\n",
    "\n",
    "# ![](https://i.gyazo.com/5949b8dc0ca6f25024ce826ed0b78ee9.png)\n",
    "\n",
    "# ![](https://i.gyazo.com/d6337ff6ed686d92dd1052beac6f25d2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式の定義を見て、気づかなかったが、この分母にある正規化定数$Z_G$は無限に発散するかもらしい、理由は$\\Sigma^*$が可算だけど、無限集合のため。\n",
    "\n",
    "**すなわち、energy modelが正規化可能かは正規化定数が有限であるかを確認しなきゃいけない。**\n",
    "\n",
    "この章で言いたいことは言語空間上でsumとって1にならないやつも正規化できれば、言語モデルになりうるよみたいなことを言いたい？\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Locally Normalized Language Models**\n",
    "\n",
    "パッと見た感じ、global modelよりもlocal modelの方が難しそう。\n",
    "\n",
    "Local modelは1step 前の文章を条件付きで考え、次のalphabet or 単語を考える操作をする。$p_{\\mathrm{LN}}(y \\mid \\boldsymbol{y})$\n",
    "\n",
    "これはなんか一見とっつきにくいけど、正規化するときに$\\Sigma^*$ではなく$\\Sigma$でsummationするから、正規化発散問題を回避できることに長けている。\n",
    "\n",
    "しかしながら、このlocal modelだと{EOS}をちゃんと定義しないと、無限列できてしまい、tightなmodelにならないことが序盤でも説明されてたので、新しい、集合を定義する。\n",
    "\n",
    "$$\n",
    "\\bar{\\Sigma} \\stackrel{\\text { def }}{=} \\Sigma \\cup\\{\\mathrm{EOS}\\} .\n",
    "$$\n",
    "\n",
    "次にsequence modelとLoccally normalized language modelの定義をする。 NLPでよく見る言語モデルの定義があって、なんでこのような定義をするのかと疑問に思っていたが、次の定義で腑に落ちました。\n",
    "\n",
    "# ![](https://i.gyazo.com/9a793239e72c32f12f7c482568084878.png)\n",
    "# ![](https://i.gyazo.com/dd1ce99259142cc82184ea65cdbe121e.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "次に、どんな言語モデルでもLoccally normalizedで表された式に書き換えることをできるらしい。\n",
    "\n",
    "そのためにprefix probabilityを定義する、例えばある単語Aがあるとし、Aから始まる文字列(途中でも可)はどれくらい妥当かを確率で測る指標。\n",
    "\n",
    "# ![](https://i.gyazo.com/5e8d73fdc36185960ac6147c53acd2a9.png)\n",
    "\n",
    "次に定理を下に示す。\n",
    "\n",
    "# ![](https://i.gyazo.com/30094230ecfb3973e1c156e5841aa51c.png)\n",
    "\n",
    "*証明*\n",
    "\n",
    "$$\n",
    "p_{\\mathrm{SM}}(y \\mid \\boldsymbol{y}) \\stackrel{\\text { def }}{=} \\frac{\\pi(\\boldsymbol{y} y)}{\\pi(\\boldsymbol{y})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{\\mathrm{SM}}(\\operatorname{EOS} \\mid \\boldsymbol{y}) \\stackrel{\\text { def }}{=} \\frac{p_{\\mathrm{LM}}(\\boldsymbol{y})}{\\pi(\\boldsymbol{y})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{\\mathrm{LN}}(\\boldsymbol{y}) & =\\left[\\prod_{t=1}^T p_{\\mathrm{SM}}\\left(y_t \\mid \\boldsymbol{y}_{<t}\\right)\\right] p_{\\mathrm{SM}}(\\operatorname{EOS} \\mid \\boldsymbol{y}) \\\\\n",
    "& =\\frac{\\pi\\left(y_1\\right)}{\\pi(\\varepsilon)} \\frac{\\pi\\left(y_1 y_2\\right)}{\\frac{\\pi\\left(y_1\\right)}{\\pi(\\boldsymbol{y}<T-1)}} \\cdots \\frac{\\pi(\\boldsymbol{y}<T)}{\\pi(\\boldsymbol{y}<T)} p_{\\mathrm{SM}}(\\operatorname{EOS} \\mid \\boldsymbol{y}) \\\\\n",
    "& =\\frac{\\pi(\\boldsymbol{y})}{\\pi(\\varepsilon)} \\frac{p_{\\mathrm{LM}}(\\boldsymbol{y})}{\\pi(\\boldsymbol{y})} \\\\\n",
    "& =p_{\\mathrm{LM}}(\\boldsymbol{y})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi(\\epsilon)$が1になるのはLanguage Modelがtightだから、自明。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
